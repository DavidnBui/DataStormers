{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import hvplot.pandas\n",
    "import pandas as pd\n",
    "from utils import load_config, fetch_api_data, write_to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.spc.noaa.gov/wcm/#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.ncdc.noaa.gov/swdiws\n",
    "# 'nx3tvs'       - (Point)   NEXRAD Level-3 Tornado Vortex Signatures\n",
    "# 'nx3meso'      - (Point)   NEXRAD Level-3 Mesocyclone Signatures\n",
    "# 'nx3hail'      - (Point)   NEXRAD Level-3 Hail Signatures\n",
    "# 'nx3structure' - (Point)   NEXRAD Level-3 Storm Cell Structure Information\n",
    "# 'warn'         - (Polygon) Severe Thunderstorm, Tornado, Flash Flood and Special Marine warnings\n",
    "datasets = [\"nx3tvs\"]\n",
    "outputFormat = \"json\"\n",
    "daterange = \"20240701:20240731\"  # \"periodOfRecord\"\n",
    "\n",
    "for dataset in datasets:\n",
    "    base_url = f\"https://www.ncdc.noaa.gov/swdiws/{outputFormat}/{dataset}/{daterange}\"\n",
    "    filename = f\"swdiws_{dataset}.csv\"\n",
    "\n",
    "    data = fetch_api_data(base_url)\n",
    "    #print(json.dumps(data, indent=2))\n",
    "    #print(data)\n",
    "\n",
    "    if data and \"result\" in data:\n",
    "        write_to_csv(data[\"result\"], filename, \"w\")\n",
    "    else:\n",
    "        print(f\"No data found or invalid response format for dataset: {dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'nx3tvs'       - (Point)   NEXRAD Level-3 Tornado Vortex Signatures\n",
    "# 'nx3meso'      - (Point)   NEXRAD Level-3 Mesocyclone Signatures\n",
    "# 'nx3hail'      - (Point)   NEXRAD Level-3 Hail Signatures\n",
    "# 'nx3structure' - (Point)   NEXRAD Level-3 Storm Cell Structure Information\n",
    "# 'warn'         - (Polygon) Severe Thunderstorm, Tornado, Flash Flood and Special Marine warnings\n",
    "datasets = [\"nx3tvs\"]\n",
    "outputFormat = \"geojson\"\n",
    "daterange = \"20240701:20240731\"  # \"periodOfRecord\"\n",
    "numResults = 2500\n",
    "\n",
    "# Initialize an empty list to store merged data\n",
    "merged_data_list = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    base_url = f\"https://www.ncdc.noaa.gov/swdiws/{outputFormat}/{dataset}/{daterange}/{numResults}\"\n",
    "    filename = f\"swdiws_{dataset}_{outputFormat}.csv\"\n",
    "\n",
    "    data = fetch_api_data(base_url)\n",
    "    # Data is nested, retrieve \"features\" dictionary\n",
    "    rows = data[\"features\"]\n",
    "    \n",
    "    # Iterate over each record in rows as that is nested as well\n",
    "    for record in rows:\n",
    "       # Merge the 'properties' and 'geometry' dictionaries\n",
    "        merged_data = {**record[\"properties\"], **record[\"geometry\"]}\n",
    "        # Append the merged data to the list\n",
    "        merged_data_list.append(merged_data)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    merged_df = pd.DataFrame(merged_data_list)\n",
    "\n",
    "# Split the coordinates column into latitude and longitude\n",
    "merged_df[['longitude', 'latitude']] = pd.DataFrame(merged_df['coordinates'].tolist(), index=merged_df.index)\n",
    "# Drop the original coordinates column\n",
    "merged_df = merged_df.drop(columns=['coordinates'])\n",
    "merged_df.head(25)\n",
    "\n",
    "    # if data and \"features\" in data:\n",
    "    #     write_to_csv(data[\"features\"], filename, \"w\")\n",
    "    # else:\n",
    "    #     print(f\"No data found or invalid response format for dataset: {dataset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'MXDV' to numeric, forcing errors to NaN if conversion fails\n",
    "merged_df['MXDV'] = pd.to_numeric(merged_df['MXDV'], errors='coerce')\n",
    "# Ensure 'WSR_ID' is treated as a string\n",
    "merged_df['WSR_ID'] = merged_df['WSR_ID'].astype(str)\n",
    "merged_df.to_csv('swdiws_nx3tvs_geojson.csv', index=False)\n",
    "print(merged_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.fema.gov/about/openfema/data-sets\n",
    "base_url = f\"https://www.fema.gov/api/open/v2/\"\n",
    "\n",
    "params = {\"$count\": \"true\",\n",
    "          \"$filter\": \"incidentType eq 'Tornado'\"}\n",
    "\n",
    "endpoint = \"DisasterDeclarationsSummaries\"\n",
    "filename = f\"{endpoint}.csv\"\n",
    "endpoint_url = f\"{base_url}{endpoint}\"\n",
    "\n",
    "data = fetch_api_data(endpoint_url, params)\n",
    "#print(json.dumps(data, indent=2))\n",
    "tornado_summary_df = pd.DataFrame(data[\"DisasterDeclarationsSummaries\"])\n",
    "write_to_csv(data[\"DisasterDeclarationsSummaries\"], filename, \"w\")\n",
    "\n",
    "disaster_numbers = tornado_summary_df.disasterNumber.unique()\n",
    "formatted_disaster_numbers = ', '.join(f'{num}' for num in disaster_numbers)\n",
    "formatted_disaster_numbers\n",
    "\n",
    "    #disasterNumber\n",
    "#tornado_newer_df = tornado_df[tornado_df[\"declarationDate\"] > \"2019-12-31\"]\n",
    "#tornado_df.groupby(by=\"disasterNumber\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"$count\": \"true\",\n",
    "          \"$filter\": f\"disasterNumber in ({formatted_disaster_numbers})\"}\n",
    "\n",
    "endpoint = \"HousingAssistanceOwners\"\n",
    "filename = f\"{endpoint}.csv\"\n",
    "endpoint_url = f\"{base_url}{endpoint}\"\n",
    "data = fetch_api_data(endpoint_url, params)\n",
    "#print(json.dumps(data, indent=2))\n",
    "housing_assistance_df = pd.DataFrame(data[\"HousingAssistanceOwners\"])\n",
    "write_to_csv(data[\"HousingAssistanceOwners\"], filename, \"w\")\n",
    "housing_assistance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "# Configure the map plot\n",
    "tornadoes_plot = merged_df.hvplot.points(\n",
    "    \"longitude\",\n",
    "    \"latitude\",\n",
    "    geo=True,\n",
    "    tiles=\"OSM\",\n",
    "    frame_width=800,\n",
    "    frame_height=600,\n",
    "    size=\"MXDV\",\n",
    "    color=\"WSR_ID\"\n",
    ")\n",
    "\n",
    "# Display the map\n",
    "tornadoes_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### National Centers for Environmental Information ###\n",
    "\n",
    "# Load config file\n",
    "config = load_config('config.json')\n",
    "\n",
    "# Get API key\n",
    "weather_api_key = config.get('National Centers for Environmental Information', {}).get('key')\n",
    "\n",
    "# Set base URL\n",
    "base_url = f\"https://www.ncei.noaa.gov/cdo-web/api/v2\"\n",
    "\n",
    "# Set endpoints\n",
    "endpoints = [\"datasets\", \"datacategories\", \"datatypes\", \"locationcategories\", \"locations\", \"stations\", \"data\"]\n",
    "\n",
    "# Check if key exists in config file \n",
    "if not weather_api_key:\n",
    "    print(\"Weather API key not found in the configuration file.\")\n",
    "    sys.exit()\n",
    "# Define headers\n",
    "headers = {\"token\": weather_api_key}\n",
    "# Define HTTPS call timeout (seconds)\n",
    "timeout = 30\n",
    "\n",
    "# Loop through each endpoint\n",
    "for endpoint in endpoints:\n",
    "    print(f\"Running: {endpoint}\")\n",
    "    \n",
    "    # Update params if the endpoint is 'data'\n",
    "    if endpoint == \"data\":\n",
    "        params = {\"datasetid\": \"GHCND\"}\n",
    "        timeout = 60\n",
    "    else:\n",
    "        # Ensure 'datasetid' is not in params for other endpoints\n",
    "        params = {\"limit\": 1000}\n",
    "\n",
    "    # Define output filename\n",
    "    filename = f\"NCEI_{endpoint}.csv\"\n",
    "    \n",
    "    # Set URL\n",
    "    url_endpoint = f\"{base_url}/{endpoint}\"\n",
    "    \n",
    "    # Run a request using params and header dictionaries\n",
    "    data = fetch_api_data(url_endpoint, params, headers, \"GET\", timeout)\n",
    "    \n",
    "    # Process and write the data to CSV\n",
    "    if data and \"results\" in data:\n",
    "        write_to_csv(data[\"results\"], filename, \"w\")\n",
    "    else:\n",
    "        print(f\"No data found or invalid response format for endpoint: {endpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API to convert coordinates to addresses\n",
    "lat = \"41.79445\"\n",
    "lng = \"-72.89675\"\n",
    "\n",
    "params = {\"format\": \"json\",\n",
    "          \"lat\": lat,\n",
    "          \"lon\": lng}\n",
    "\n",
    "headers = {\"User-Agent\": \"MyCustomApp/1.0 (hef1125@hotmail.com)\"}\n",
    "\n",
    "endpoint = \"reverse\"\n",
    "\n",
    "base_url = f\"https://nominatim.openstreetmap.org/{endpoint}\"\n",
    "\n",
    "\n",
    "data = fetch_api_data(base_url, params, headers)\n",
    "print(json.dumps(data, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
